\documentclass[reprint, nobalancelastpage, amsmath,amssymb,aps,%pra,%prb,%rmp,%prstab,
%prstper,%floatfix,
] {revtex4-1}

\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{subcaption}
\usepackage[labelformat=parens,labelsep=quad,skip=3pt]{caption}
\usepackage{blindtext}


\usepackage[]{hyperref}
\usepackage[]{physics}
\usepackage[]{listings}
\usepackage[T1]{fontenc}
\usepackage{color}


%------------------------------------------------

\begin{document}

\title{Project 1 - FYS4150}

\author{Sajjad Ahmadigoltapeh, Einar Aurbakken, Bastian Skjelstad, Per Harald Barkost}

\affiliation{email-address: sajjadah@uio.no, eanorway@gmail.com, bastiabs@uio.no, ph.barkost@gmail.com}

\date{\today}\\

\begin{abstract}

\begin{centering}
\url{https://github.com/einaur/fys4150/tree/master/Project1/main_code/C}
\url{https://github.com/einaur/fys4150/tree/master/Project1/main_code/Python}

\end{centering}
\vspace{10pt}


We solve the one-dimensional Poisson equation with Dirichlet boundary conditions using Gaussian elimination. We use a solver for a general tridiagonal matrix, and then develop a specialized solver for the case of a symmetric Toeplitz matrix. Computations with different step-size shows that the relative error decreases with smaller step-size only until a certain limit. Comparison with a LU decomposition solver shows that the Gaussian solver specialized for tridigaonal matrices are significantly faster.
\end{abstract}

\maketitle

\section{\label{sec:Int}Introduction}
Linear differential equation with the form of $u''(x) = f(x)$ is an ordinary equation in the scientific problems and Poisson's equation which is a wellknown second order differential equation in electromagnetism follows above mentioned form. In this study two solutions will be introduced for Poisson's equation which via using C++ programming language. First, one dimensional Poisson equation with Dirichlet boundary conditions was analyzed and solved with tridiagonal matrix algorithm. Afterward, LU decomposition method was applied to solve the same differential equation. Both methods are tested for different sizes of matrix and their relevant relative errors were calculated.



\section{\label{sec:Met} Methodology}

\subsection{\label{sec:Pro} Stating the problem}

Poisson's equation is defined as equation(1).

\begin{equation}
\laplacian\Phi=-4\pi \rho(\vb{r})
\end{equation}

Using spherical dimension, the $\laplacian$ operator is defined as equation(2)

\begin{equation}
\laplacian=\frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d\Phi}{dr} \right)
\end{equation}

Using a spherically symmetric assumption simplifies above equation to equation(3) which depends only on radius $r$ :

\begin{equation}
\frac{d^2\phi}{dr^2}=-4\pi r\rho(r)
\end{equation}

which is generally is presented as:

\begin{equation}
-u''(x) = f(x), \qquad x \in (0,1), \qquad u(0)=u(1)=0
\end{equation}\\
Above mentioned boundary condition is called Dirichlet boundary condition.
To employ Gaussian elimination method, first step is discretizing the domain of $x$. In fact, using Taylor expansion helps differential equation(4) is approximated by a linear equation with an error of ($\mathcal{O}(n^2)$). Namely, second order differential equation(4) now is presented as equation(5)

\begin{equation}
	-\frac{u_{i+1} + u_{i-1} - 2u_{i}}{h^{2}} = f_{i} \qquad \text{for } i = 1,2,...,n
\end{equation}\\
such that, mesh size is given as: $h = 1/(n+1)$ and $x = ih$ and relevant boundary conditions are $u_{0} = u_{n+1} = 0$, where $f_{i} = f(x_{i})$. Generally, a set of generated linear equations mathematically are shown as: \\
\begin{equation}
	\bm{A}\bm{u} = \bm{\tilde{b}}
\end{equation}\\

where A is a tridiagonal matrix i.e.

\begin{equation}
A =
\begin{pmatrix}
  2 & -1 & 0 & \cdots & \cdots & 0 \\
  -1 & 2 &  -1 & 0 &\cdots & \cdots \\
  0 & -1 & 2 & -1 & 0 & \cdots \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
  0 & \cdots & \cdots & -1 & 2 & -1 \\
  0 & \cdots & \cdots & \cdots & -1 & 2
\end{pmatrix}
\end{equation}

and $\tilde{b_{i}} = -h^{2}f_{i}$. Then, floating points and relevant timing of algorithm is tested for $n = 10, 100, 1000$ grid points.\\
To estimate the relative error of applied method, $f(x)$ should be calculated via numerical and exact solutions. Hence, first of all set of linear equations which is derived as equation (5) is solved. Then by supposing $f(x) = 100e^{-10x}$ for equation (4) that, have exact solution of: $u(x) = 1 - (1-e^{-10})x - e^{-10x}$ it will be possible to estimate  relative error.\\

\subsection{\label{sec:Gen}General algorithm}
General tridiagonal matrix is shown as equation (8):

\begin{equation}
	\bm{A}\bm{u} = \begin{pmatrix}
  a_{1} & b_{1} & 0 & \cdots & \cdots & 0 \\
  c_{2} & a_{2} &  b_{2} & 0 &\cdots & \cdots \\
  0 & c_{3} & a_{3} & b_{3} & 0 & \cdots \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
  0 & \cdots & \cdots & c_{n-1} & a_{n-1} & b_{n-1} \\
  0 & \cdots & \cdots & \cdots & c_{n} & a_{n}
\end{pmatrix}
\begin{pmatrix}
	u_{1} \\
	u_{2} \\
	\vdots \\
	\\
	\vdots\\
	u_{n}
\end{pmatrix} =
\begin{pmatrix}
	d_{1} \\
	d_{2} \\
	\vdots \\
	\\
	\vdots\\
	d_{n}
\end{pmatrix}
\end{equation}

Using Gaussian elimination provides forward substitution as:

\begin{equation}
	\bm{A}\bm{u} = \begin{pmatrix}
  \tilde{a_{1}} & b_{1} & 0 & \cdots & \cdots & 0 \\
  0 & \tilde{a_{2}} &  b_{2} & 0 &\cdots & \cdots \\
  0 & 0 & \tilde{a_{3}} & b_{3} & 0 & \cdots \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
  0 & \cdots & \cdots & 0 & \tilde{a_{n-1}} & b_{n-1} \\
  0 & \cdots & \cdots & \cdots & 0 & a_{n}
\end{pmatrix}
\begin{pmatrix}
	u_{1} \\
	u_{2} \\
	\vdots \\
	\\
	\vdots\\
	u_{n}
\end{pmatrix} =
\begin{pmatrix}
	\tilde{d_{1}} \\
	\tilde{d_{2}} \\
	\vdots \\
	\\
	\vdots\\
	\tilde{d_{n}}
\end{pmatrix}
\end{equation}

where $\tilde{a_{1}} = a_{1}$ and

\begin{equation}
\tilde{a_{i}} = a_{i} - b_{i}c_{i}/\tilde{a_{i-1}}, \qquad i=2,3,...,n
\end{equation}

and for right hand side:
\begin{equation}
\tilde{d_{i}} = d_{i} - c_{i}\tilde{d_{i-1}}/\tilde{a_{i}}, \qquad i=2,3,...,n
\end{equation}

The backward substitution i.e. $u_{n} = \tilde{d_{n}}/ \tilde{a_{n}}$
gives:
\begin{equation}
u_{i} = (\tilde{d_{i}} - b_{i}u_{i+1})/\tilde{a_{i}}, \qquad i=n-1,n-2,...,1
\end{equation}

An important point for each algorithm which should be considered is how efficient the algorithm is. Therefore, it is worth to find how many floating point is required for an algorithm. In fact, for forward substitution 6 FLOPS for each iteration is counted which becomes $6*(n-1)$ FLOPS for whole iteration. Furthermore, $\mathcal{O}(8n)$ FLOPS is required for backward substitution.

\subsection{\label{sec:Spe}LU decomposition method}
LU decomposition technique let write a non-singular matrix $A$ as a product of L and U
\begin{equation}
	A = LU
\end{equation}

where L is a lower triangular matrix

\begin{equation}
L =
\begin{pmatrix}
  l_{11} & 0 & 0 & \cdots & \cdots & 0 \\
  l_{21} & l_{22} &  0 & 0 &\cdots & \cdots \\
  l_{31} & l_{32} & l_{33} & 0 & 0 & \cdots \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
  \cdots & \cdots & \cdots & \cdots & l_{n-1,n-1} & 0 \\
  0 & \cdots & \cdots & \cdots & l_{n,n-1} & l_{nn}
\end{pmatrix}
\end{equation}

and U is a upper triangular matrix

\begin{equation}
U =
\begin{pmatrix}
u_{11} & u_{12} & u_{13} & \dots & u_{1n-1} & u_{1n} \\
0 & u_{22} & u_{23} & \dots & u_{2n-1} & u_{2n} \\
0 & 0 & u_{33} & \dots & u_{3n-1} & u_{3n} \\
  &\vdots & & \ddots & \vdots  & \\
0 & 0 & 0 & \dots & u_{n-1n-1} & u_{n-1n} \\
0 & 0 & 0 & \dots & 0 & u_{nn}
\end{pmatrix}
\end{equation}

and finally A could be substituted with L and U as following:

(**note: to prevent any confusion for LU decomposition $u$ matrix in equation 6 is  replaced by $v$)
\begin{equation}
	\bm{Av} = \bm{LUv} = \tilde{\bm{b}}
\end{equation}
Therefore, two sets of linear equations are generated as bellow for finding $\bm{v}$:
\begin{equation}
	\bm{Uv} = \bm{y} \quad\quad\quad	\bm{Ly} = \tilde{\bm{b}}
\end{equation}

Number of FLOPS for LU-decomposition method is \cite{golub}
\begin{equation}
	\bm{N_{LU}} = \bm{\frac{2}{3}n^{3}-2n^{2}}=\bm{\mathcal{O}(\frac{2}{3}n^{3})}
\end{equation}
It means for a matrix with order of $10^{5}$ number of FLOPS equal to $\frac{2}{3}\cdot10^{15}$.

\subsection{\label{sec:Err}Error estimation}
Equation 19 shows a formula that is used to calculate relative error and compare result of numerical solution with exact solution:

\begin{equation}
	\epsilon_{i} = \log_{10}(|\frac{v_{i}-u_{i}}{u_{i}}|)
\end{equation}

The error should be calculated as a function of step length $h$ (grid points $n$). Then, its max value is plotted as a function of $h$, and this provides the maximum produced error by numerical solution.\\



\section{\label{sec:Res}Results and discussion}
\subsection{\label{sec:err}Numerical and analytical comparison}
General numerical solution and analytical solutions are both computed for matrices of the size $10\times10$, $100\times100$, $1000\times1000$ then, results of $n = 10$ and $n = 100$ are plotted as figure \ref{fig:plotn10} and figure \ref{fig:plotn100} respectively. By comparing figure \ref{fig:plotn10} and figure \ref{fig:plotn100} it is realized that for large n, results of numerical solution is getting closer to exact solution and in contrary, by decreasing n, deviation of numerical from exact solution increases.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/plotn10.png}
  \caption{Comparison of numerical and exact solution for n=10}
  \label{fig:plotn10}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/plotn100.png}
  \caption{Comparison of numerical and exact solution for n=100}
  \label{fig:plotn100}
\end{figure}

\subsection{\label{sec:err}Error analysis}
Relative error was calculated according to equation (19) for $n = 10, 10^2, 10^3, 10^4, 10^5, 10^6$ and $10^7$ then, maximum relative errors with respect to step size is plotted as figure \ref{fig:max-rel-value.png}. As can be seen, by increasing n, error decreases up to about $n = 10^5$, then rises with higher slope. When the value of $n$ is further increased, the relative error will become larger. This increase in error for values of $n > 10^5$ is due to the accumulation of round-off errors. When these round-off errors accumulate, they will contribute largely to giving an inaccurate numerical approximation.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/max-rel-value.png}
  \caption{Maximum relative value}
  \label{fig:max-rel-value.png}
\end{figure}

\subsection{\label{sec:err}Run time analysis}

\begin{table}[h]
\caption{Run time of different algorithm}
\begin{tabular}{lccc}
\hline
n & General[s] & Specific[s] & LU \\ \hline
10 & 0.0000231 & 0.0000147 & 0.00049471\\
100 & 0.0002930 & 0.0001308 & 0.00042295\\
1000 & 0.0019359 & 0.0011398 & 0.02385902\\
10000 & 0.0281178 & 0.0116548 & 16.0864861\\
100000 & 0.2144536 & 0.1143908 &      n/a \\
1000000 & 2.2559521 & 1.2527589 &     n/a  \\
10000000 & 20.312860 & 12.374752 &    n/a
\end{tabular}
\label{tab:run_time}
\end{table}\\

 To test the efficiency of different methods program run time for general tridiagonal, specific tridiagonal and LU decomposition algorithm is calculated and summarized in table \ref{tab:run_time}. As can be seen, compared with general algorithm, the computation took less time with specific algorithm. Furthermore, compared with LU decomposition-based solver, the tridiagonal Gaussian methods uses significantly less time for all values of n. In addition, for n > 10000, run time for LU is not accessible because the computer did not have enough memory. However, general technique still works and just spent around 20 seconds for calculations.\\
\section{Conclusion}
Overall,specific algorithm is faster than general algorithm and LU decomposition is expensive solution in terms of time and required memory capacity. Furthermore, increasing $n$ lead to accumulation in error.

\\
\\
\\
\begin{thebibliography}{10}
    \bibitem{morten}{Hjorth-Jensen, M. (2015). \emph{Computational
        Physics - Lecture Notes 2018}. University of Oslo}
    \bibitem{golub}{Golub, G.H., van Loan, C.F. (1996). \emph{Matrix
         Computations} (3rd ed.), Baltimore: John Hopkins.}
\end{thebibliography}



\end{document}
